{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Feature Engineering in NLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy"
   ]
  },
  {
   "source": [
    "## Data Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(197, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "text_df = pd.read_csv('data/txt/News articles/blaise.txt', header=None, sep=';', names=['text'])\n",
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      Once the network is trained, we can feed in a ...\n",
       "1      Convolutional neural networks are very general...\n",
       "2      Modern, sophisticated machine learning techniq...\n",
       "3      Every machine learning system has parameters —...\n",
       "4      One technical pitfall to guard against is over...\n",
       "                             ...                        \n",
       "192    [21] In this statistic “black” and “white” bot...\n",
       "193    [22] Racial implicit bias is measured using re...\n",
       "194              [23] Their subjects are all Chinese men\n",
       "195    [24] This research, as well as the lack of evi...\n",
       "196    [25] Malcolm Gladwell’s book Blink popularized...\n",
       "Name: text, Length: 197, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "text_df['text']"
   ]
  },
  {
   "source": [
    "## Create SpaCy English Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# empty list\n",
    "docs = []\n",
    "for text in text_df['text']:\n",
    "    doc = nlp(text)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "source": [
    "## Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Once the network is trained, we can feed in a photo, and we get out the year in which the system guesses it was taken. For example, for the following two photos ChronoNet guesses 1951 (left) and 1971 (right):', 'Convolutional neural networks are very general and very powerful. As an example, consider Ilya Kostrikov and Tobias Weyand’s ChronoNet, a CNN that guesses the year in which a photo was taken. Since public sources can provide large numbers of digitally archived photos taken over the past century with known dates, it’s relatively straightforward to obtain labeled data (dated photos, in this case) with which to train this network.', 'Modern, sophisticated machine learning techniques like convolutional neural networks (CNNs) have many millions of parameters, hence need a great deal of training data to avoid overfitting. Obtaining enough labelled data to both train and test a system is often the greatest practical challenge facing a machine learning researcher.', 'Every machine learning system has parameters — or there is nothing to learn. Simple systems may have only a handful. Increasing the number of parameters can allow a system to learn more complex relationships, making for a more powerful learner and, if the relationships between input and output are complex, a lower error rate. On the other hand, more parameters also allow a system to memorize more of the training data, hence overfit more easily. This means that there is a relationship between the number of parameters and the amount of training data needed.', 'One technical pitfall to guard against is overfitting . This happens when the machine is able to memorize the right answers to individual training examples without generalizing, meaning learning an underlying pattern that will hold when tested on different data. The simplest way to avoid overfitting is simply to test the performance of the system on a random subset of the labelled data that is “held out”, meaning not used during training. If the system’s performance on this test data is roughly as good as on the training data, then one can feel confident that the system really has learned how to see a general pattern in the data, and hasn’t just memorized the training examples. This is the same as the rationale for giving students a midterm exam with questions they haven’t seen before, rather than just reusing examples that have been worked through in class.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.text for token in docs]\n",
    "print(tokens[0:5])"
   ]
  },
  {
   "source": [
    "## Lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10242"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "lemmas = [token.lemma_ for doc in docs for token in doc]\n",
    "len(lemmas)"
   ]
  },
  {
   "source": [
    "## Removing non-alpha chars using stopwords"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8455"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "lemmas_alpha = [lemma for lemma in lemmas if lemma.isalpha() or lemma == '-PRON-']\n",
    "\n",
    "len(lemmas_alpha)"
   ]
  },
  {
   "source": [
    "## Stopwords"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4579"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "lemmas_cleaned = [lemma for lemma in lemmas_alpha if lemma not in stopwords]\n",
    "len(lemmas_cleaned)"
   ]
  },
  {
   "source": [
    "## Putting it all together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      network train feed photo year system guess exa...\n",
       "1      convolutional neural network general powerful ...\n",
       "2      modern sophisticated machine learning techniqu...\n",
       "3      machine learning system parameter learn simple...\n",
       "4      technical pitfall guard overfitte happen machi...\n",
       "                             ...                        \n",
       "192    statistic black white exclude population ident...\n",
       "193    racial implicit bias measure use reaction time...\n",
       "194                                  subject chinese man\n",
       "195    research lack evidence accuracy impression rev...\n",
       "196    Malcolm Gladwell book Blink popularize idea sn...\n",
       "Name: cleaned, Length: 197, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    # lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    lemmas_cleaned = [lemma for lemma in lemmas if lemma.isalpha() \\\n",
    "        and lemma not in stopwords]\n",
    "    return ' '.join(lemmas_cleaned)\n",
    "\n",
    "text_df['cleaned'] = text_df['text'].apply(preprocess)\n",
    "text_df['cleaned']"
   ]
  },
  {
   "source": [
    "## Part-of-speech tagging"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_cleaned = []\n",
    "for text in text_df['cleaned']:\n",
    "    doc_cleaned = nlp(text)\n",
    "    docs_cleaned.append(doc_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4349\n[('network', 'NOUN'), ('train', 'NOUN'), ('feed', 'NOUN'), ('photo', 'NOUN'), ('year', 'NOUN'), ('system', 'NOUN'), ('guess', 'VERB'), ('example', 'NOUN'), ('follow', 'NOUN'), ('photo', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "pos = [(token.text, token.pos_) for doc in docs_cleaned for token in doc]\n",
    "\n",
    "print(len(pos))\n",
    "print(pos[:10])"
   ]
  },
  {
   "source": [
    "## Named entity recognition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "224\n[('ChronoNet', 'ORG'), ('convolutional neural network', 'ORG'), ('Ilya Kostrikov Tobias Weyand', 'FAC'), ('CNN', 'ORG'), ('past century', 'DATE'), ('cnn', 'ORG'), ('era pervasive camera', 'ORG'), ('AI', 'GPE'), ('Wu Zhang', 'PERSON'), ('Xiaolin Wu', 'PERSON'), ('Inference Criminality', 'ORG'), ('arXiv', 'NORP'), ('November', 'DATE'), ('Wu Zhang', 'PERSON'), ('rapid development artificial intelligence', 'ORG'), ('new era', 'ORG'), ('algorithm bias', 'PERSON'), ('today', 'DATE'), ('Italians', 'NORP'), ('Italians', 'NORP')]\n"
     ]
    }
   ],
   "source": [
    "ner = [(ent.text, ent.label_) for doc in docs_cleaned for ent in doc.ents]\n",
    "\n",
    "print(len(ner))\n",
    "print(ner[:20])"
   ]
  },
  {
   "source": [
    "## Bag of words and n-gram"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  \\\n",
       "0  Once the network is trained, we can feed in a ...   \n",
       "1  Convolutional neural networks are very general...   \n",
       "2  Modern, sophisticated machine learning techniq...   \n",
       "3  Every machine learning system has parameters —...   \n",
       "4  One technical pitfall to guard against is over...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  network train feed photo year system guess exa...  \n",
       "1  convolutional neural network general powerful ...  \n",
       "2  modern sophisticated machine learning techniqu...  \n",
       "3  machine learning system parameter learn simple...  \n",
       "4  technical pitfall guard overfitte happen machi...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Once the network is trained, we can feed in a ...</td>\n      <td>network train feed photo year system guess exa...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Convolutional neural networks are very general...</td>\n      <td>convolutional neural network general powerful ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Modern, sophisticated machine learning techniq...</td>\n      <td>modern sophisticated machine learning techniqu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Every machine learning system has parameters —...</td>\n      <td>machine learning system parameter learn simple...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>One technical pitfall to guard against is over...</td>\n      <td>technical pitfall guard overfitte happen machi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "source": [
    "### Bag of Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "text_vec = cvec.fit_transform(text_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   cvec_ability  cvec_able  cvec_abnormal  cvec_abolitionist  cvec_absolutely  \\\n",
       "0             0          0              0                  0                0   \n",
       "1             0          0              0                  0                0   \n",
       "2             0          0              0                  0                0   \n",
       "3             0          0              0                  0                0   \n",
       "4             0          1              0                  0                0   \n",
       "\n",
       "   cvec_abstract  cvec_accelerate  cvec_access  cvec_accomplish  cvec_accord  \\\n",
       "0              0                0            0                0            0   \n",
       "1              0                0            0                0            0   \n",
       "2              0                0            0                0            0   \n",
       "3              0                0            0                0            0   \n",
       "4              0                0            0                0            0   \n",
       "\n",
       "   ...  cvec_wrong  cvec_wrongly  cvec_wu  cvec_xi  cvec_xiaolin  cvec_year  \\\n",
       "0  ...           0             0        0        0             0          1   \n",
       "1  ...           0             0        0        0             0          1   \n",
       "2  ...           0             0        0        0             0          0   \n",
       "3  ...           0             0        0        0             0          0   \n",
       "4  ...           0             0        0        0             0          0   \n",
       "\n",
       "   cvec_yellow  cvec_yes  cvec_young  cvec_zhang  \n",
       "0            0         0           0           0  \n",
       "1            0         0           0           0  \n",
       "2            0         0           0           0  \n",
       "3            0         0           0           0  \n",
       "4            0         0           0           0  \n",
       "\n",
       "[5 rows x 1648 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cvec_ability</th>\n      <th>cvec_able</th>\n      <th>cvec_abnormal</th>\n      <th>cvec_abolitionist</th>\n      <th>cvec_absolutely</th>\n      <th>cvec_abstract</th>\n      <th>cvec_accelerate</th>\n      <th>cvec_access</th>\n      <th>cvec_accomplish</th>\n      <th>cvec_accord</th>\n      <th>...</th>\n      <th>cvec_wrong</th>\n      <th>cvec_wrongly</th>\n      <th>cvec_wu</th>\n      <th>cvec_xi</th>\n      <th>cvec_xiaolin</th>\n      <th>cvec_year</th>\n      <th>cvec_yellow</th>\n      <th>cvec_yes</th>\n      <th>cvec_young</th>\n      <th>cvec_zhang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1648 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "cvec_df = pd.DataFrame(text_vec.toarray(), columns=cvec.get_feature_names()).add_prefix('cvec_')\n",
    "\n",
    "cvec_df.head()"
   ]
  },
  {
   "source": [
    "### n-grams"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-grams\n",
    "bigrams_model = CountVectorizer(stop_words='english', ngram_range= (2,2))\n",
    "\n",
    "two_grams = bigrams_model.fit_transform(text_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   two-grams_ability read  two-grams_ability recognize  \\\n",
       "0                       0                            0   \n",
       "1                       0                            0   \n",
       "2                       0                            0   \n",
       "3                       0                            0   \n",
       "4                       0                            0   \n",
       "\n",
       "   two-grams_ability tend  two-grams_able ask  two-grams_able date  \\\n",
       "0                       0                   0                    0   \n",
       "1                       0                   0                    0   \n",
       "2                       0                   0                    0   \n",
       "3                       0                   0                    0   \n",
       "4                       0                   0                    0   \n",
       "\n",
       "   two-grams_able guess  two-grams_able intuitive  two-grams_able learn  \\\n",
       "0                     0                         0                     0   \n",
       "1                     0                         0                     0   \n",
       "2                     0                         0                     0   \n",
       "3                     0                         0                     0   \n",
       "4                     0                         0                     0   \n",
       "\n",
       "   two-grams_able memorize  two-grams_able reliably  ...  \\\n",
       "0                        0                        0  ...   \n",
       "1                        0                        0  ...   \n",
       "2                        0                        0  ...   \n",
       "3                        0                        0  ...   \n",
       "4                        1                        0  ...   \n",
       "\n",
       "   two-grams_zhang claim  two-grams_zhang criminal  \\\n",
       "0                      0                         0   \n",
       "1                      0                         0   \n",
       "2                      0                         0   \n",
       "3                      0                         0   \n",
       "4                      0                         0   \n",
       "\n",
       "   two-grams_zhang experiment  two-grams_zhang guess  two-grams_zhang host  \\\n",
       "0                           0                      0                     0   \n",
       "1                           0                      0                     0   \n",
       "2                           0                      0                     0   \n",
       "3                           0                      0                     0   \n",
       "4                           0                      0                     0   \n",
       "\n",
       "   two-grams_zhang paper  two-grams_zhang relate  two-grams_zhang result  \\\n",
       "0                      0                       0                       0   \n",
       "1                      0                       0                       0   \n",
       "2                      0                       0                       0   \n",
       "3                      0                       0                       0   \n",
       "4                      0                       0                       0   \n",
       "\n",
       "   two-grams_zhang use  two-grams_zhang work  \n",
       "0                    0                     0  \n",
       "1                    0                     0  \n",
       "2                    0                     0  \n",
       "3                    0                     0  \n",
       "4                    0                     0  \n",
       "\n",
       "[5 rows x 3695 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>two-grams_ability read</th>\n      <th>two-grams_ability recognize</th>\n      <th>two-grams_ability tend</th>\n      <th>two-grams_able ask</th>\n      <th>two-grams_able date</th>\n      <th>two-grams_able guess</th>\n      <th>two-grams_able intuitive</th>\n      <th>two-grams_able learn</th>\n      <th>two-grams_able memorize</th>\n      <th>two-grams_able reliably</th>\n      <th>...</th>\n      <th>two-grams_zhang claim</th>\n      <th>two-grams_zhang criminal</th>\n      <th>two-grams_zhang experiment</th>\n      <th>two-grams_zhang guess</th>\n      <th>two-grams_zhang host</th>\n      <th>two-grams_zhang paper</th>\n      <th>two-grams_zhang relate</th>\n      <th>two-grams_zhang result</th>\n      <th>two-grams_zhang use</th>\n      <th>two-grams_zhang work</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 3695 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "two_grams_df = pd.DataFrame(two_grams.toarray(), columns=bigrams_model.get_feature_names()).add_prefix('two-grams_')\n",
    "\n",
    "two_grams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "two-grams_machine learning      23\n",
       "two-grams_wu zhang              23\n",
       "two-grams_non criminal          14\n",
       "two-grams_machine learn         12\n",
       "two-grams_deep learning         11\n",
       "two-grams_untrustworthy face     8\n",
       "two-grams_facial appearance      8\n",
       "two-grams_learning technique     7\n",
       "two-grams_human judge            7\n",
       "two-grams_face image             6\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# most common words\n",
    "two_grams_df.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-grams\n",
    "trigrams_model = CountVectorizer(stop_words='english', ngram_range= (3,3))\n",
    "\n",
    "three_grams = trigrams_model.fit_transform(text_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   three-grams_ability read intention  \\\n",
       "0                                   0   \n",
       "1                                   0   \n",
       "2                                   0   \n",
       "3                                   0   \n",
       "4                                   0   \n",
       "\n",
       "   three-grams_ability recognize criminality  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "\n",
       "   three-grams_ability tend creative  three-grams_able ask computer  \\\n",
       "0                                  0                              0   \n",
       "1                                  0                              0   \n",
       "2                                  0                              0   \n",
       "3                                  0                              0   \n",
       "4                                  0                              0   \n",
       "\n",
       "   three-grams_able date photo  three-grams_able guess gender  \\\n",
       "0                            0                              0   \n",
       "1                            0                              0   \n",
       "2                            0                              0   \n",
       "3                            0                              0   \n",
       "4                            0                              0   \n",
       "\n",
       "   three-grams_able intuitive sense  three-grams_able learn subtler  \\\n",
       "0                                 0                               0   \n",
       "1                                 0                               0   \n",
       "2                                 0                               0   \n",
       "3                                 0                               0   \n",
       "4                                 0                               0   \n",
       "\n",
       "   three-grams_able memorize right  three-grams_able reliably distinguish  \\\n",
       "0                                0                                      0   \n",
       "1                                0                                      0   \n",
       "2                                0                                      0   \n",
       "3                                0                                      0   \n",
       "4                                1                                      0   \n",
       "\n",
       "   ...  three-grams_zhang paper automated  three-grams_zhang paper illustrate  \\\n",
       "0  ...                                  0                                   0   \n",
       "1  ...                                  0                                   0   \n",
       "2  ...                                  0                                   0   \n",
       "3  ...                                  0                                   0   \n",
       "4  ...                                  0                                   0   \n",
       "\n",
       "   three-grams_zhang paper purport  three-grams_zhang relate work  \\\n",
       "0                                0                              0   \n",
       "1                                0                              0   \n",
       "2                                0                              0   \n",
       "3                                0                              0   \n",
       "4                                0                              0   \n",
       "\n",
       "   three-grams_zhang result consistent  three-grams_zhang result exactly  \\\n",
       "0                                    0                                 0   \n",
       "1                                    0                                 0   \n",
       "2                                    0                                 0   \n",
       "3                                    0                                 0   \n",
       "4                                    0                                 0   \n",
       "\n",
       "   three-grams_zhang use id  three-grams_zhang use label  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "2                         0                            0   \n",
       "3                         0                            0   \n",
       "4                         0                            0   \n",
       "\n",
       "   three-grams_zhang use scare  three-grams_zhang work use  \n",
       "0                            0                           0  \n",
       "1                            0                           0  \n",
       "2                            0                           0  \n",
       "3                            0                           0  \n",
       "4                            0                           0  \n",
       "\n",
       "[5 rows x 3818 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>three-grams_ability read intention</th>\n      <th>three-grams_ability recognize criminality</th>\n      <th>three-grams_ability tend creative</th>\n      <th>three-grams_able ask computer</th>\n      <th>three-grams_able date photo</th>\n      <th>three-grams_able guess gender</th>\n      <th>three-grams_able intuitive sense</th>\n      <th>three-grams_able learn subtler</th>\n      <th>three-grams_able memorize right</th>\n      <th>three-grams_able reliably distinguish</th>\n      <th>...</th>\n      <th>three-grams_zhang paper automated</th>\n      <th>three-grams_zhang paper illustrate</th>\n      <th>three-grams_zhang paper purport</th>\n      <th>three-grams_zhang relate work</th>\n      <th>three-grams_zhang result consistent</th>\n      <th>three-grams_zhang result exactly</th>\n      <th>three-grams_zhang use id</th>\n      <th>three-grams_zhang use label</th>\n      <th>three-grams_zhang use scare</th>\n      <th>three-grams_zhang work use</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 3818 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "three_grams_df = pd.DataFrame(three_grams.toarray(), columns=trigrams_model.get_feature_names()).add_prefix('three-grams_')\n",
    "\n",
    "three_grams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "three-grams_machine learning technique        5\n",
       "three-grams_criminal non criminal             5\n",
       "three-grams_trustworthy untrustworthy face    4\n",
       "three-grams_convolutional neural network      4\n",
       "three-grams_image non criminal                3\n",
       "three-grams_wu zhang criminal                 3\n",
       "three-grams_wu zhang claim                    3\n",
       "three-grams_valla et al                       3\n",
       "three-grams_wu zhang use                      3\n",
       "three-grams_social perception face            3\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# most common words\n",
    "three_grams_df.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "source": [
    "## Tfidf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tvec_guess        0.485483\n",
       "tvec_photo        0.379790\n",
       "tvec_feed         0.322695\n",
       "tvec_follow       0.322695\n",
       "tvec_leave        0.299310\n",
       "tvec_chrononet    0.242741\n",
       "tvec_train        0.242741\n",
       "tvec_right        0.242741\n",
       "tvec_network      0.242741\n",
       "tvec_year         0.219356\n",
       "Name: 0, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tvec = tfidf.fit_transform(text_df['cleaned'])\n",
    "\n",
    "tvec_df = pd.DataFrame(tvec.toarray(), columns=tfidf.get_feature_names()).add_prefix('tvec_')\n",
    "\n",
    "tvec_df.iloc[0].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}